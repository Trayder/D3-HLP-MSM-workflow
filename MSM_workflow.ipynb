{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSM workflow\n",
    "###### Thomas, T.; Yuriev, E; Chalmers, D. K. \"MarkovState Model Analysis of Haloperidol Binding to the D3 Dopamine Receptor.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = '/home/msm_data/'\n",
    "%cd /home/msm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mdtraj as md\n",
    "from mdtraj.geometry import compute_distances, compute_angles\n",
    "\n",
    "import os,sys,shutil\n",
    "import re\n",
    "\n",
    "from msmbuilder.cluster import *\n",
    "from msmbuilder.msm import *\n",
    "from msmbuilder.dataset import dataset\n",
    "from msmbuilder.featurizer import *\n",
    "from msmbuilder import utils\n",
    "from msmbuilder.decomposition import tICA, PCA\n",
    "from msmbuilder.lumping import *\n",
    "from msmbuilder.tpt import mfpts\n",
    "from msmbuilder import hmm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.patches import Ellipse\n",
    "#plot graphs inline, e.g. don't need plot.show()\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import mixture\n",
    "from itertools import product, chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose dataset location (relative to working directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make sure any molecules of interest in system.pdb are whole.\n",
    "dcd files may be faster to read than xtc due to compression'''\n",
    "ds = dataset('center_fit/.dcd', topology='system.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Fitting trajectories is important for analysis, but raw (non-fit)\n",
    "trajectories are necessary as starting structures for subsquent runs'''\n",
    "ds_nofit = dataset('no_fit/*.dcd', topology='system.pdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guess pass number based on existing folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_pass = 1\n",
    "while os.path.exists(\"pass\"+str(next_pass).zfill(2)):\n",
    "    next_pass += 1\n",
    "this_pass = next_pass-1\n",
    "pass_folder = \"pass\"+str(this_pass).zfill(2)+\"/\"\n",
    "print(next_pass, this_pass, pass_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_folder = \"./pass01/\"\n",
    "next_pass, this_pass= None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_featurization(output_folder=None, ds=None, featurizer=None):\n",
    "    '''Generic featurization function\n",
    "    featurizes trajectories in chunks to lower memory usage\n",
    "    smaller chunks use less memory but featurize slower'''\n",
    "    out_dataset = ds.create_derived(output_folder, fmt='dir-npy')\n",
    "\n",
    "    for key in ds.keys():\n",
    "        trajectory = []\n",
    "        for i, chunk in enumerate(ds.iterload(key, chunk=10000)):\n",
    "            trajectory.append(featurizer.partial_transform(chunk))\n",
    "        out_dataset[key] = np.concatenate(trajectory)\n",
    "        out_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''It is safest to process trajectories and verify results before loading into MSMBuilder.\n",
    "AtomIndices_ca.dat should be 0-indexed, 1 number per line.\n",
    "This can be conveniently extracted from gromacs ndx files\n",
    "Alternatively pass a list from mdtraj, e.g. traj.topology.select(\"name CA\")\n",
    "'''\n",
    "ref_traj = dataset('system.pdb')[0]\n",
    "featurizer = RawPositionsFeaturizer(atom_indices=np.loadtxt('AtomIndices_ca.dat',dtype=int), ref_traj=ref_traj)\n",
    "make_featurization(pass_folder+\"protein_rawpos_featurized\",featurizer=featurizer, ds=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_featurized = dataset(pass_folder+\"protein_rawpos_featurized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Contacts.dat is 0 indexed, 1 pair per line\n",
    "This can be conveniently extracted from gromacs ndx files\n",
    "Alternatively pass a list from mdtraj'''\n",
    "featurizer = ContactFeaturizer(contacts=np.loadtxt('Contacts.dat'), scheme='closest-heavy', ignore_nonprotein=False)\n",
    "make_featurization(pass_folder+\"ligand_contact_featurized\",featurizer=featurizer, ds=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dataset = dataset(pass_folder+\"ligand_contact_featurized\")\n",
    "output_dataset = parent_dataset.create_derived(pass_folder+'bin_contact_featurized')\n",
    "new_key = 0\n",
    "for key in parent_dataset.keys():\n",
    "    output_dataset[new_key] = np.where(parent_dataset[key] > 0.6, 0., 1.)\n",
    "    output_dataset.close()\n",
    "    new_key += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_contact_featurized = dataset(pass_folder+'bin_contact_featurized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hbond featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hbonds_fingerprint(traj, input_triplets=None, periodic=True):\n",
    "'''\n",
    "Currently only works for a provided list of hbonds (input_triplets)\n",
    "'''\n",
    "    distance_cutoff = 0.25            # nanometers\n",
    "    angle_cutoff = 2.0 * np.pi / 3.0  # radians\n",
    "\n",
    "    if traj.topology is None:\n",
    "        raise ValueError('baker_hubbard requires that traj contain topology '\n",
    "                         'information')\n",
    "\n",
    "    angle_triplets = input_triplets\n",
    "    distance_pairs = angle_triplets[:, [1,2]]  # possible H..acceptor pairs\n",
    "\n",
    "    angles = compute_angles(traj, angle_triplets, periodic=periodic)\n",
    "    distances = compute_distances(traj, distance_pairs, periodic=periodic)\n",
    "    mask = np.logical_and(distances < distance_cutoff, angles > angle_cutoff)\n",
    "\n",
    "    return np.where(mask,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bonds(itp_file, topology):\n",
    "    \"\"\"\n",
    "    Read the bonds from an itp file and apply them to a similarly\n",
    "    indexed MDTraj topology (e.g. both itp and topology should be ligand only)\n",
    "    \"\"\"\n",
    "    assert topology.n_bonds == 0,\"topology already has bonds\"\n",
    "    with open(itp_file) as f:\n",
    "        #do nothing until [ bonds ]\n",
    "        for line in f:\n",
    "            if line.strip() == '[ bonds ]':\n",
    "                break\n",
    "        for line in f:\n",
    "            #skip blank lines or lines starting with ;\n",
    "            if ((not line.strip()) or (line[0] == ';')):\n",
    "                continue\n",
    "            #stop when reaching [ pairs ]\n",
    "            if line.strip() == '[ pairs ]':\n",
    "                break\n",
    "            atom1,atom2 = line.split()[0:2]\n",
    "            topology.add_bond(topology.atom(int(atom1)-1),topology.atom(int(atom2)-1))\n",
    "    return topology     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_hbonds(ds=None, ligresname=None, chosen_hbonds=None):\n",
    "    hbonds_featurized = dataset(pass_folder+'hbonds_featurized', mode='a', fmt='dir-npy')\n",
    "    for key in ds.keys():   \n",
    "        print(key,)\n",
    "        trajectory = []\n",
    "        for i, traj in enumerate(ds.iterload(key, chunk=4001)):\n",
    "            ligand_top  = traj.topology.subset(traj.topology.select(ligresname)\n",
    "            protein_top = traj.topology.subset(traj.topology.select('not '+ligresname))\n",
    "            ligand_top = populate_bonds(itpfile, ligand_top)\n",
    "            traj.topology = protein_top.join(ligand_top)\n",
    "            trajectory.append(calc_hbonds_fingerprint(traj, input_triplets=chosen_hbonds))\n",
    "\n",
    "        hbonds_featurized[key] = np.concatenate(trajectory)\n",
    "    return hbonds_featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use this cell to manually specify hydrogen bonds (by index)\n",
    "\"\"\"\n",
    "\n",
    "#Indices for all acceptor/donor combinations between ligand and inwardly oriented non-loop residues\n",
    "#Protonated amine is excluded as an acceptor\n",
    "#No back-bone hydrogen bonds as loops are excluded\n",
    "chosen_hbonds = np.vstack([[57, 58, 2730],\n",
    " [57, 58, 2766],\n",
    " [1547, 1548, 2730],\n",
    " [1547, 1548, 2766],\n",
    " [1555, 1556, 2730],\n",
    " [1555, 1556, 2766],\n",
    " [1579, 1580, 2730],\n",
    " [1579, 1580, 2766],\n",
    " [2193, 2194, 2730],\n",
    " [2193, 2194, 2766],\n",
    " [2220, 2221, 2730],\n",
    " [2220, 2221, 2766],\n",
    " [2220, 2222, 2730],\n",
    " [2220, 2222, 2766],\n",
    " [2353, 2354, 2730],\n",
    " [2353, 2354, 2766],\n",
    " [2361, 2362, 2730],\n",
    " [2361, 2362, 2766],\n",
    " [2384, 2385, 2730],\n",
    " [2384, 2385, 2766],\n",
    " [2438, 2439, 2730],\n",
    " [2438, 2439, 2766],\n",
    " [2730, 2729, 57],\n",
    " [2730, 2729, 559],\n",
    " [2730, 2729, 560],\n",
    " [2730, 2729, 761],\n",
    " [2730, 2729, 762],\n",
    " [2730, 2729, 1547],\n",
    " [2730, 2729, 1555],\n",
    " [2730, 2729, 1579],\n",
    " [2730, 2729, 2188],\n",
    " [2730, 2729, 2193],\n",
    " [2730, 2729, 2219],\n",
    " [2730, 2729, 2220],\n",
    " [2730, 2729, 2353],\n",
    " [2730, 2729, 2361],\n",
    " [2730, 2729, 2384],\n",
    " [2730, 2729, 2438],\n",
    " [2739, 2737, 57],\n",
    " [2739, 2737, 559],\n",
    " [2739, 2737, 560],\n",
    " [2739, 2737, 761],\n",
    " [2739, 2737, 762],\n",
    " [2739, 2737, 1547],\n",
    " [2739, 2737, 1555],\n",
    " [2739, 2737, 1579],\n",
    " [2739, 2737, 2188],\n",
    " [2739, 2737, 2193],\n",
    " [2739, 2737, 2219],\n",
    " [2739, 2737, 2220],\n",
    " [2739, 2737, 2353],\n",
    " [2739, 2737, 2361],\n",
    " [2739, 2737, 2384],\n",
    " [2739, 2737, 2438]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbonds_featurized = featurize_hbonds(ds=ds, ligresname='resname LIG', chosen_hbonds=chosen_hbonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine ligand features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = bin_contact_featurized.create_derived(pass_folder+'bin_contact_hbond_featurized')\n",
    "for key in bin_contact_featurized.keys():\n",
    "    output_dataset[key] = np.column_stack((bin_contact_featurized[key],hbonds_featurized[key]))\n",
    "    output_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_featurized = dataset(pass_folder+'/bin_contact_hbond_featurized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tICA Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Choice of lag time is generally robust, take your best guess.\n",
    "Overestimate n_components and then decide correct number based on graphs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_prot = tICA(n_components=10,lag_time=10).fit(protein_featurized)\n",
    "tica_prot_transformed = tica_prot.transform(protein_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tica_prot.timescales_, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tica_prot.timescales_[:-1]/tica_prot.timescales_[1:], 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_prot = tICA(n_components=1,lag_time=10).fit(protein_featurized)\n",
    "tica_prot_transformed = tica_prot.transform(protein_featurized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tICA Ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Choice of lag time is generally robust, take your best guess.\n",
    "Overestimate n_components and then decide correct number based on graphs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_lig = tICA(n_components=10,lag_time=10).fit(ligand_featurized)\n",
    "tica_lig_transformed = tica_lig.transform(ligand_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tica_lig.timescales_, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tica_lig.timescales_[:-1]/tica_lig.timescales_[1:], 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_lig = tICA(n_components=3,lag_time=10).fit(ligand_featurized)\n",
    "tica_lig_transformed = tica_lig.transform(ligand_featurized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine tICAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_transformed = []\n",
    "for dims, more_dims in zip(tica_lig_transformed, tica_prot_transformed):\n",
    "    tica_transformed.append(np.column_stack((dims, more_dims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tica_transformed[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.dump(tica_transformed, pass_folder+\"tica_transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica_transformed = utils.load(pass_folder+\"tica_transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose either Hierarchical Agglomerative Clustering (Much slower, high memory usage) or KMeans (very fast, slightly worse) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "clustering_landmark = LandmarkAgglomerative(n_clusters=30,\n",
    "n_landmarks=len(ds.glob_matches)*5).fit(np.asarray(tica_transformed))\n",
    "clustering_landmark.labels_ = clustering_landmark.transform(tica_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''clustered is equivalent to clustering.labels_'''\n",
    "clustering = KMeans(n_clusters=20, n_jobs=-1).fit(np.asarray(tica_transformed))\n",
    "clustered = clustering.transform(tica_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.dump(clustered, pass_folder+\"clustered\")\n",
    "utils.dump(clustering, pass_folder+\"clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = utils.load(pass_folder+\"clustering\")\n",
    "clustered = utils.load(pass_folder+\"clustered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(clustering=None, tica_transformed=None, chosen_states='all'):\n",
    "    '''plots clusters as binned histograms of the distance in tica-space of\n",
    "    each cluster member to the cluster center\n",
    "    Useful to check cluster quality'''\n",
    "    tica_transformed = np.asarray(tica_transformed)\n",
    "    states_done = 0\n",
    "    n_states = max(map(lambda x: max(x), clustering.labels_)) + 1\n",
    "    n_states_2 = len(np.unique(np.concatenate(clustering.labels_)))\n",
    "    assert n_states == n_states_2, \"Must have non-empty, zero-indexed, consecutive states: found %d states and %d unique states.\" % (n_states, n_states_2)\n",
    "                                                                                                                                                           \n",
    "    if chosen_states == 'all':\n",
    "        chosen_states = range(n_states)\n",
    "        \n",
    "    def calc_pair_deviation(clustering=clustering, state=None, tica_transformed=tica_transformed):\n",
    "        all_frames = [np.where(a == state)[0] for a in clustering.labels_]\n",
    "        pairs = [(trj, frame) for (trj, frames) in enumerate(all_frames) for frame in frames]\n",
    "        pair_deviation = [np.sqrt(np.sum(np.subtract(tica_transformed[pair[0]][pair[1]], clustering.cluster_centers_[state])**2)) for pair in pairs]\n",
    "        return pair_deviation\n",
    "    \n",
    "    fig, sandwich = plt.subplots(figsize=(18,6))\n",
    "    fig.delaxes(fig.axes[0])\n",
    "    for state in chosen_states:\n",
    "        columns = 3\n",
    "        plot_num = len(fig.axes)+1\n",
    "        rows = (plot_num-1)/columns+1\n",
    "        \n",
    "        if plot_num%columns == 1 and plot_num > columns:\n",
    "            plt.show()\n",
    "            fig, sandwich = plt.subplots(figsize=(18,6))\n",
    "            fig.delaxes(fig.axes[0])\n",
    "            rows = 1\n",
    "            plot_num = 1\n",
    "            \n",
    "            #fig.set_size_inches(fig.get_figwidth(), (fig.get_figheight()*(float(rows)/(rows-1))))        \n",
    "        \n",
    "        subplot = fig.add_subplot(rows, columns, plot_num)\n",
    "\n",
    "        #for i in range(plot_num):\n",
    "        #    fig.axes[i].change_geometry(rows, columns , i+1)\n",
    "\n",
    "        pair_deviation = calc_pair_deviation(state=state)\n",
    "        subplot.set_title(\"State\"+str(state))\n",
    "        subplot.set_xlim(0,2)\n",
    "        subplot.hist(pair_deviation, bins=20)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(clustering, tica_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct MSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_implied_timescales(clustering=None, n_timescales=10, max_lagtime=None, log=True, n_points=10):\n",
    "    '''Creates markov models for a range of lag times and plots their implied timescales'''\n",
    "    timescale_step = max_lagtime // n_points\n",
    "    lag_times = range(1,max_lagtime,timescale_step)\n",
    "    \n",
    "    models = []\n",
    "    for lag_time in lag_times:\n",
    "        models.append(MarkovStateModel(lag_time=lag_time, verbose=True, ergodic_cutoff = 'on', n_timescales=n_timescales).fit(clustering.labels_))\n",
    "    timescales = [m.timescales_ for m in models]\n",
    "    #errors = np.asarray([m.uncertainty_timescales() for m in models])\n",
    "    n_timescales = min(n_timescales, min(len(ts) for ts in timescales))\n",
    "    timescales = np.array([ts[:n_timescales] for ts in timescales])\n",
    "    plt.cla()\n",
    "    for i in range(1,n_timescales):\n",
    "        try:\n",
    "            timescale, = plt.plot(lag_times, timescales[:,i], 'o-')\n",
    "            #plt.fill_between(lag_times, timescales[:,i]-errors[:,i], timescales[:,i]+errors[:,i], facecolor=timescale.get_color(), alpha=0.5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.fill_between(range(max_lagtime),range(max_lagtime),1)\n",
    "    if log==True:\n",
    "        plt.semilogy()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_implied_timescales(clustering, max_lagtime=700, n_timescales=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Choose a lag time of 1 during early phases to ensure states are not excised'''\n",
    "MSM = MarkovStateModel(lag_time=400, n_timescales=10, ergodic_cutoff='on').fit(clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot tICA free-energy surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tica(tica_transformed=None, HMM=None, clustering=None, dims='all', states=None, trajectories=[]):\n",
    "    '''Provide clustering OR HMM\n",
    "    dims: list of indices. All combinations will be plotted as separate graphs\n",
    "    states: list of indices. Cluster center of each specifiedstate is plotted\n",
    "    trajectories: list of indices (refer to ds.glob_matches). Trajectory path is plotted as a line white->blue'''\n",
    "    \n",
    "    if dims == 'all':\n",
    "        dims = range(len(tica_transformed[0][0]))\n",
    "    if HMM:\n",
    "        cluster_centers = HMM.means_\n",
    "    if clustering:\n",
    "        cluster_centers = clustering.cluster_centers_\n",
    "    if (states == 'all'):\n",
    "        states = range(len(cluster_centers))\n",
    "        \n",
    "    def make_plots(tica_transformed, states, trajectories, dim1=None, dim2=None, color='blue'):\n",
    "           \n",
    "        x = np.concatenate(tica_transformed)[:,dim1]\n",
    "        y = np.concatenate(tica_transformed)[:,dim2]\n",
    "        \n",
    "        z,x,y = np.histogram2d(x,y, bins=50)\n",
    "        F = -np.log(z)\n",
    "        extent = [x[0], x[-1], y[0], y[-1]]\n",
    "        axes[nplots%2].set_title(\"tIC \"+str(dim1)+\" vs. tIC \"+str(dim2))\n",
    "        #adjust levels to define contour lines\n",
    "        axes[nplots%2].contourf(F.T, 50, cmap=plt.cm.hot, extent=extent, levels=np.linspace(-9,0,10))\n",
    "        \n",
    "        if states:\n",
    "                a = cluster_centers[states,dim1]\n",
    "                b = cluster_centers[states,dim2]  \n",
    "                axes[nplots%2].plot(a,b,\"o\",zorder=1)\n",
    "        \n",
    "        for traj in trajectories:\n",
    "            u = tica_transformed[traj][:,dim1]\n",
    "            v = tica_transformed[traj][:,dim2]\n",
    "            points = np.array([u, v]).T.reshape(-1, 1, 2)\n",
    "            segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "            lc = LineCollection(segments, cmap=plt.get_cmap('Blues'))\n",
    "            lc.set_array(np.asarray(range(len(u))))\n",
    "            axes[nplots%2].plot()\n",
    "            axes[nplots%2].add_collection(lc)\n",
    "            axes[nplots%2].axis('auto')        \n",
    "\n",
    "    nplots = 0\n",
    "    for dim_num, dim1 in enumerate(dims):\n",
    "        for dim2 in dims[dim_num+1:]:\n",
    "            if nplots%2 == 0:\n",
    "                fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,9))\n",
    "\n",
    "            make_plots(tica_transformed, states, trajectories, dim1=dim1, dim2=dim2, color='blue')\n",
    "            \n",
    "            if nplots%2==1: plt.show()\n",
    "            nplots += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tica(tica_transformed, clustering=clustering, HMM=None, states='all', trajectories=[], dims='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse MSM and choose metric that will determine next-pass structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate simple connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_transmin(MSM=MSM, n_results=10):\n",
    "'''Calculates the number of transitions into and out of each state\n",
    "    returns the lower number (in vs out) for each state\n",
    "    Effectively identifies poor connectivity on an individual state basis'''    \n",
    "    trans_min = np.zeros(len(MSM.countsmat_))\n",
    "    for state in range(len(MSM.countsmat_)):\n",
    "        trans_self = MSM.countsmat_[state,state]\n",
    "        trans_out  = np.sum(MSM.countsmat_[state,:]) - trans_self\n",
    "        trans_in   = np.sum(MSM.countsmat_[:,state]) - trans_self\n",
    "        trans_min[state]  = min(trans_out, trans_in)\n",
    "\n",
    "    print(trans_min.argsort()[:n_results])\n",
    "    return trans_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_min = calc_transmin(MSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in trans_min.argsort()[:10]:\n",
    "    print(\"State \"+str(state)+\" population = \"+str(np.unique(np.concatenate(clustering.labels_),return_counts=True)[1][state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MFPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfpt_matrix = mfpts(MSM)\n",
    "'''Specify solvent state manually if necessary'''\n",
    "solvent_state = MSM.mapping_[clustering.labels_[0][0]]\n",
    "mfpt_list = mfpt_matrix[solvent_state]\n",
    "\n",
    "mfpt_states = (-mfpt_list).argsort()[:10]\n",
    "print(mfpt_states)\n",
    "chosen_states_mfpt = mfpt_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_mfpt(transmat, start_state=None, sink_states=None, n_iter=100, lag_time=1):\n",
    "    '''Simulates movement through the transition matrix and returns a list of first passage times\n",
    "    between start_state (int) and sink_states (list) for calculating mean, median and distribution\n",
    "    of first passage times\n",
    "    mfpt is calculated in time units of the transmat, it is returned multiplied by lag_time,\n",
    "    choose a number that will scale to your desired units\n",
    "    With sufficient iterations will result in same mfpt as calculated from transition path theory\n",
    "    \n",
    "    Returns:\n",
    "    list of first passage times\n",
    "    \n",
    "    Can easily be modded to return a list of transitions, for movie making etc.\n",
    "    '''\n",
    "\n",
    "    transprob = copy.copy(transmat)\n",
    "    #create a copy of transmat with cumulative probabilities for the transitions out of each state\n",
    "    #to enable use of np.searchsorted to randomly select transitions\n",
    "    for i,state in enumerate(transmat):\n",
    "        for j,transition in enumerate(state):\n",
    "            transprob[i,j] = np.sum(transmat[i,:j+1])\n",
    "    nstep_list = []\n",
    "    for i in range(n_iter):\n",
    "        state = start_state\n",
    "        nsteps = 0\n",
    "        while (state not in sink_states):\n",
    "            nsteps +=1\n",
    "            random_number = random.random()\n",
    "            new_state = np.searchsorted(transprob[state],random_number)\n",
    "        #     if (new_state != state):\n",
    "        #         print(new_state)\n",
    "            state = new_state\n",
    "        #     if (nsteps > 100):\n",
    "        #         break\n",
    "        nstep_list.append(nsteps)\n",
    "    return np.asarray(nstep_list)*lag_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep_list = advanced_mfpt(HMM.transmat_, start_state=17, sink_states=[13], lag_time=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(nstep_list))\n",
    "print(np.mean(nstep_list))\n",
    "plt.hist(np.asarray(nstep_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate per-state contribution to eigenvalue uncertainty"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'''Requires custom function, see cell below\n",
    "'''\n",
    "uncertainty = MSM.uncertainty_eigenvalues_per_state()\n",
    "for timescale in range(len(uncertainty)-1):\n",
    "    print (-uncertainty[timescale+1]).argsort()[:10], uncertainty[timescale+1][(-uncertainty[timescale+1]).argsort()[:6]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Method used to calculate each states uncertainty to the eigenvalues in:\n",
    "# Thomas, T.; Yuriev, E; Chalmers, D. K.\n",
    "# MarkovState Model Analysis of Haloperidol Binding to the D3 Dopamine Receptor\".\n",
    "# This is a modified version of the existing method MarkovStateModel.uncertainty_eigenvalues()\n",
    "# Add this function to 'class MarkovStateModel', in the 'msm' module of the 'msmbuilder' package.\n",
    "\n",
    "def uncertainty_eigenvalues_per_state(self):\n",
    "    \"\"\"Estimate of the element-wise asymptotic standard deviation\n",
    "    in the model eigenvalues per state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigma_eigs : np.array, shape=(n_timescales+1, n_states)\n",
    "        The estimated symptotic standard deviation in the eigenvalues per state.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Hinrichs, Nina Singhal, and Vijay S. Pande. \"Calculation of\n",
    "       the distribution of eigenvalues and eigenvectors in Markovian state\n",
    "       models for molecular dynamics.\" J. Chem. Phys. 126.24 (2007): 244101.\n",
    "    \"\"\"\n",
    "    if self.reversible_type is None:\n",
    "        raise NotImplementedError('reversible_type must be \"mle\" or \"transpose\"')\n",
    "\n",
    "    n_timescales = min(self.n_timescales, self.n_states_ - 1)\n",
    "    if n_timescales is None:\n",
    "        n_timescales = self.n_states_ - 1\n",
    "    u, lv, rv = self._get_eigensystem()\n",
    "\n",
    "    sigma2 = np.zeros((n_timescales + 1,self.n_states_))\n",
    "    for k in range(n_timescales + 1):\n",
    "        dLambda_dT = np.outer(lv[:, k], rv[:, k])\n",
    "        for i in range(self.n_states_):\n",
    "            ui = self.countsmat_[:, i]\n",
    "            wi = np.sum(ui)\n",
    "            cov = wi*np.diag(ui) - np.outer(ui, ui)\n",
    "            quad_form = dLambda_dT[i].dot(cov).dot(dLambda_dT[i])\n",
    "            sigma2[k,i] = quad_form / (wi**2*(wi+1))\n",
    "    return np.sqrt(sigma2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'''Pick 3 states with the highest error contribution for each of the 10 slowest timescales\n",
    "Only unique states are kept so this can practically range between 10 and 30 states'''\n",
    "chosen_states = []\n",
    "for timescale in range(len(uncertainty)-1):\n",
    "    for state in (-uncertainty[timescale+1]).argsort()[:3]:\n",
    "        chosen_states.append(state)\n",
    "chosen_states = np.unique(chosen_states)\n",
    "print(str(len(chosen_states))+\" states chosen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_clusters(counts_matrix, final_clusters=None, populations=None):\n",
    "    '''\n",
    "    Finds the highest minimum value between forward and backward transitions...\n",
    "    Merges most connected pairs of states together and reports \n",
    "    the <final_clusters> least connected states\n",
    "    \n",
    "    Also note that due to discretization error, there is a bias towards joining\n",
    "    clusters into the largest blob rather than forming multiple aggregates.\n",
    "    This bias only really emerges after the largest connectivity issues are fixed\n",
    "    '''\n",
    "    \n",
    "    counts_matrix = counts_matrix.copy()\n",
    "    diagonal = np.diagonal(counts_matrix).copy()\n",
    "    np.fill_diagonal(counts_matrix, 0)\n",
    "    min_matrix = counts_matrix.copy()\n",
    "    \n",
    "    cluster_record = [[i] for i in range(counts_matrix.shape[0])]\n",
    "    n_clusters = counts_matrix.shape[0]\n",
    "\n",
    "    while n_clusters > final_clusters:\n",
    "        \n",
    "        for i in range(len(min_matrix)):\n",
    "            for j in range(i+1,len(min_matrix)):\n",
    "                min_matrix[i,j] = min(counts_matrix[i,j], counts_matrix[j,i])\n",
    "                min_matrix[j,i] = min_matrix[i,j]\n",
    "        \n",
    "        x = np.argmax(min_matrix)//min_matrix.shape[0]\n",
    "        y = np.argmax(min_matrix) - x*min_matrix.shape[0]\n",
    "\n",
    "        #print(\"merging \"+str(y)+\" into \"+str(x))\n",
    "\n",
    "        counts_matrix[x] = counts_matrix[x]+counts_matrix[y]\n",
    "        counts_matrix[y] = counts_matrix[y]*0\n",
    "        counts_matrix[:,x] = counts_matrix[:,x]+counts_matrix[:,y]\n",
    "        counts_matrix[:,y] = counts_matrix[:,y] * 0\n",
    "\n",
    "        diagonal = diagonal + np.diagonal(counts_matrix)\n",
    "        diagonal[x] = diagonal[x] + diagonal[y]\n",
    "        diagonal[y] = 0\n",
    "        np.fill_diagonal(counts_matrix, 0)\n",
    "\n",
    "        cluster_record[x] = cluster_record[x] + cluster_record[y]\n",
    "        cluster_record[y] = [-1]\n",
    "\n",
    "        n_clusters -= 1\n",
    "        \n",
    "        print(\"*LUMP*\")\n",
    "        for i in cluster_record:\n",
    "            if (i != [-1]) & (len(i)>1):\n",
    "                print(i)\n",
    "\n",
    "    for i in range(len(diagonal)):\n",
    "        counts_matrix[i,i] = diagonal[i]\n",
    "\n",
    "    for i in reversed(range(len(cluster_record))):\n",
    "        if (cluster_record[i] == [-1]):\n",
    "            counts_matrix = np.delete(counts_matrix, i, 0)\n",
    "            counts_matrix = np.delete(counts_matrix, i, 1)\n",
    "            cluster_record = np.delete(cluster_record, i)\n",
    "            \n",
    "    reduced_matrix = counts_matrix\n",
    "            \n",
    "    return reduced_matrix, cluster_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gateways(counts_matrix, cluster_record):\n",
    "    '''Identifies the individual state in each \"lump\" which is most connected to the rest of the network.\n",
    "    '''\n",
    "    bad_lumps = cluster_record[np.asarray([len(cluster_record[i]) for i in range(len(cluster_record))]).argsort()][:-1]\n",
    "    chosen_states = np.asarray([], dtype=int)\n",
    "    for lump in bad_lumps:\n",
    "        transmax = 0\n",
    "        for cluster in lump:\n",
    "            transmat = counts_matrix.copy()\n",
    "            for cluster2 in lump:\n",
    "                transmat[cluster][cluster2] = 0\n",
    "            if np.amax(transmat[cluster]) > transmax:\n",
    "                transmax = np.amax(transmat[cluster])\n",
    "                gateway = cluster\n",
    "        chosen_states = np.append(chosen_states, gateway)\n",
    "\n",
    "    return chosen_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix, cluster_record = join_clusters(MSM.countsmat_, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cluster_record:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_states_connect = find_gateways(MSM.countsmat_, cluster_record)\n",
    "chosen_states_connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples(clustering, n_samples, featurized=None, selection_criteria='random', chosen_states='all'):\n",
    "    \"\"\"Sample conformations from each state. Samples are randomly selected to be\n",
    "    closer to the cluster center than the mean of the distribution\n",
    "                                                                                                                                                           \n",
    "    Parameters\n",
    "    ----------\n",
    "    clustering : Object\n",
    "        should have property clustering.labels_ which corresponds\n",
    "        to the cluster assignment of every frame\n",
    "\n",
    "    n_samples : int\n",
    "        How many samples to return from each state\n",
    "        \n",
    "    featurized : Object, optional\n",
    "        Featurization used for calculating distance to cluster centers\n",
    "        \n",
    "    selection_criteria : string\n",
    "        Should be one of 'median', '95percentile, 'closest', 'random'\n",
    "            median: Returns states closer to the cluster center than the median of all cluster members\n",
    "            80percentile: Returns states closer to the cluster center than 80% of all cluster members\n",
    "            closest: Returns the n_samples closest cluster members to the cluster center\n",
    "            random: random sampling with replacement\n",
    "                                                                                                                                                           \n",
    "    Returns\n",
    "    -------\n",
    "    selected_pairs_by_state : np.array, dtype=int, shape=(n_states, n_samples, 2)\n",
    "        selected_pairs_by_state[state] gives an array of randomly selected (trj, frame)\n",
    "        pairs from the specified state.\n",
    "                                                                                                                                                           \n",
    "    See Also\n",
    "    --------\n",
    "    utils.map_drawn_samples : Extract conformations from MD trajectories by index.\n",
    "                                                                                                                                                           \n",
    "    \"\"\"\n",
    "    sequences = clustering.labels_\n",
    "    random = check_random_state(None)\n",
    "    \n",
    "    n_states = max(map(lambda x: max(x), sequences)) + 1\n",
    "    n_states_2 = len(np.unique(np.concatenate(sequences)))\n",
    "    assert n_states == n_states_2, \"Must have non-empty, zero-indexed, consecutive states: found %d states and %d unique states.\" % (n_states, n_states_2)\n",
    "                                                                                                                                                           \n",
    "    if chosen_states == 'all':\n",
    "        chosen_states = range(n_states)\n",
    "\n",
    "    selected_pairs_by_state = []\n",
    "    for state in chosen_states:\n",
    "        all_frames = [np.where(a == state)[0] for a in sequences]\n",
    "        pairs = [(trj, frame) for (trj, frames) in enumerate(all_frames) for frame in frames]\n",
    "\n",
    "        if selection_criteria=='median':\n",
    "            pair_deviation = [np.sqrt(np.sum(np.subtract(featurized[pair[0]][pair[1]], clustering.cluster_centers_[state])**2)) for pair in pairs]\n",
    "            pairs = np.asarray(pairs)[np.where(pair_deviation < np.median(pair_deviation))]\n",
    "            selected_pairs_by_state.append([pairs[random.choice(len(pairs))] for i in range(n_samples)])\n",
    "            \n",
    "        if selection_criteria=='80percentile':\n",
    "            pair_deviation = [np.sqrt(np.sum(np.subtract(featurized[pair[0]][pair[1]], clustering.cluster_centers_[state])**2)) for pair in pairs]\n",
    "            pairs = np.asarray(pairs)[np.where(pair_deviation < np.percentile(pair_deviation,20))]\n",
    "            selected_pairs_by_state.append([pairs[random.choice(len(pairs))] for i in range(n_samples)])\n",
    "         \n",
    "        if selection_criteria=='closest':\n",
    "            pair_deviation = [np.sqrt(np.sum(np.subtract(featurized[pair[0]][pair[1]], clustering.cluster_centers_[state])**2)) for pair in pairs]\n",
    "            selected_pairs_by_state.append([np.asarray(pairs)[np.argsort(pair_deviation)[i]] for i in range(n_samples)])\n",
    "                        \n",
    "        if selection_criteria=='random':\n",
    "            selected_pairs_by_state.append([pairs[random.choice(len(pairs))] for i in range(n_samples)])\n",
    "            \n",
    "    return np.array(selected_pairs_by_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_data(file_path=None, MSM=None):\n",
    "    '''This function was designed to output data in the format recognised by the old MSMexplorer program\n",
    "    There has been a msmexplorer python package developed for visualizing the data which is probably better.\n",
    "    However the MSMexplorer program lets you arrange states with the mouse, which is nice'''\n",
    "    os.makedirs(file_path)\n",
    "    np.savetxt(file_path+'/Populations.dat', MSM.populations_, delimiter='\\n')\n",
    "\n",
    "    tCounts = MSM.countsmat_\n",
    "    tCountsFile=open(file_path+\"/tCounts.mtx\", \"w\")\n",
    "    line = ' '.join(map(str,np.shape(tCounts))) + ' ' + str(np.count_nonzero(tCounts)) + '\\n'\n",
    "    tCountsFile.write(line)\n",
    "    for indexes, value in np.ndenumerate(tCounts):\n",
    "        x,y = indexes\n",
    "        if (value != 0):\n",
    "            line = str(x+1) + ' ' + str(y+1) + ' ' + str(value) + \"\\n\"\n",
    "            tCountsFile.write(line)\n",
    "\n",
    "    tCountsFile.close()\n",
    "\n",
    "    tProb = MSM.transmat_\n",
    "    tProbFile=open(file_path+\"/tProb.mtx\", \"w\")\n",
    "    line = ' '.join(map(str,np.shape(tProb))) + ' ' + str(np.count_nonzero(tProb)) + '\\n'\n",
    "    tProbFile.write(line)\n",
    "    for indexes, value in np.ndenumerate(tProb):\n",
    "        x,y = indexes\n",
    "        if (value != 0):\n",
    "            line = str(x+1) + ' ' + str(y+1) + ' ' + str(value) + \"\\n\"\n",
    "            tProbFile.write(line)\n",
    "\n",
    "    tProbFile.close()\n",
    "    \n",
    "    for state, samples in enumerate(selected_pairs_by_state):\n",
    "    #for state in MSM.state_labels_:\n",
    "        samples = selected_pairs_by_state[state]\n",
    "\n",
    "        traj = None\n",
    "        for pair_num, pair in enumerate(samples):\n",
    "            print(pair_num)\n",
    "            traj_num,frame = pair\n",
    "            traj_num = traj_num\n",
    "    #        traj_num = mapping[traj_num]\n",
    "            try:\n",
    "                traj = traj.join(ds.get(traj_num)[frame])\n",
    "            except:\n",
    "                traj = ds.get(traj_num)[frame]\n",
    "        filename = str(file_path)+'/State' + str(state) + '.xtc'\n",
    "        traj.save_xtc(filename)\n",
    "        print(state,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pairs_by_state = draw_samples(clustering, 10, tica_transformed)\n",
    "print(selected_pairs_by_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure MSM has not trimmed states and use MSM.mapping_ if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data(file_path=pass_folder+\"MSM_pass\"+str(this_pass).zfill(2)+\"_\"+str(MSM.lag_time), MSM=MSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup next pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure chosen_states is not in MSM space\n",
    "\n",
    "MSMs may excise states with poor connectivity, changing the numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pairs_by_state = draw_samples(clustering, tica_transformed, 5, random=False, chosen_states=chosen_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Only adds starting structure, all other files must be in a generic\n",
    "simulation folder 'blank_sim/' in the workdir'''\n",
    "next_pass_folder = \"pass\"+str(next_pass).zfill(2)+\"/\"\n",
    "os.makedirs(next_pass_folder)\n",
    "for index,sample in enumerate(np.vstack(selected_pairs_by_state)):\n",
    "    %cd $workdir\n",
    "    traj,frame = sample\n",
    "    sim_num = index\n",
    "    run_name = 'd3_hlp_'+str(next_pass)+'_'+str(sim_num)\n",
    "    run_folder = next_pass_folder+run_name+\"/\"\n",
    "    shutil.copytree('blank_sim/', run_folder)\n",
    "    ds.get(traj)[frame].save_pdb(run_folder+'system.pdb')\n",
    "    %cd $run_folder\n",
    "    !gmx grompp -f em1.mdp -po em1_out.mdp -c system.pdb -p system.top -o em1.tpr -maxwarn 1\n",
    "    !gmx mdrun -deffnm em1\n",
    "    !gmx grompp -f md1.mdp -po md1_out.mdp -c em1.gro -p system.top -o md1.tpr -maxwarn 1\n",
    "%cd $workdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_HMM_timescales(tica_transformed=None, n_states=None, max_lagtime=2000, \n",
    "                        n_timescales=None, n_init=10, n_em_iter=10, n_steps=3):\n",
    "    '''Implied timescales but for HMMs. This can easily take forever, especially if you\n",
    "    want to make quality models.\n",
    "    Also note that HMMs have more variables than MSM and this function fixes them all, \n",
    "    and only varies lag_time'''\n",
    "    timescale_step = max_lagtime // n_steps\n",
    "    lag_times = range(1,max_lagtime,timescale_step)\n",
    "    sequences = tica_transformed\n",
    "    n_features = len(tica_transformed[0][0])\n",
    "    if n_timescales==None:\n",
    "        n_timescales=n_states-1\n",
    "    else:\n",
    "        n_timescales = min(n_timescales, n_states-1)\n",
    "        \n",
    "    hmm_timescales = np.zeros((0,n_timescales))\n",
    "    log_likelihoods = []\n",
    "    print(\"lag_time: \",)\n",
    "    for l, lag_time in enumerate(lag_times):\n",
    "        strided_data = [s[i::lag_time] for s in sequences for i in range(lag_time)]\n",
    "        HMM = hmm.GaussianHMM(n_states=n_states, n_init=n_init, n_iter=n_em_iter).fit(strided_data)\n",
    "        timescales = HMM.timescales_ * lag_time\n",
    "        hmm_timescales = np.vstack((hmm_timescales, timescales))\n",
    "        log_likelihoods.append(HMM.fit_logprob_[-1])\n",
    "        print(\" \"+str(lag_time),)\n",
    "            \n",
    "    for i in range(n_timescales):\n",
    "        timescale, = plt.plot(lag_times, hmm_timescales[:,i])\n",
    "    \n",
    "    #print log_likelihoods\n",
    "    plt.show()\n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM_draw_samples(HMM, tica_transformed, n_samples, random=False, chosen_states='all'):\n",
    "    \"\"\"Sample conformations from each state. Samples are randomly selected to be\n",
    "    closer to the cluster center than the mean of the distribution\n",
    "                                                                                                                                                           \n",
    "    Parameters\n",
    "    ----------\n",
    "    clustering : Object\n",
    "        should have property clustering.labels_ which corresponds\n",
    "        to the cluster assignment of every frame\n",
    "\n",
    "    n_samples : int\n",
    "        How many samples to return from each state\n",
    "                                                                                                                                                           \n",
    "    Returns\n",
    "    -------\n",
    "    selected_pairs_by_state : np.array, dtype=int, shape=(n_states, n_samples, 2)\n",
    "        selected_pairs_by_state[state] gives an array of randomly selected (trj, frame)\n",
    "        pairs from the specified state.\n",
    "                                                                                                                                                           \n",
    "    See Also\n",
    "    --------\n",
    "    utils.map_drawn_samples : Extract conformations from MD trajectories by index.\n",
    "                                                                                                                                                           \n",
    "    \"\"\"\n",
    "    \n",
    "    logprob = [\n",
    "        mixture.log_multivariate_normal_density(\n",
    "        x, HMM.means_, HMM.vars_, covariance_type='diag'\n",
    "        ) for x in tica_transformed\n",
    "        ]\n",
    "\n",
    "    sequences = [lp.argmax(1) for lp in logprob]\n",
    "\n",
    "    n_states = max(map(lambda x: max(x), sequences)) + 1\n",
    "    n_states_2 = len(np.unique(np.concatenate(sequences)))\n",
    "    assert n_states == n_states_2, \"Must have non-empty, zero-indexed, consecutive states: found %d states and %d unique states.\" % (n_states, n_states_2)\n",
    "                                                                                                                                                           \n",
    "    random_state = check_random_state(None)\n",
    "    \n",
    "    if chosen_states == 'all':\n",
    "        chosen_states = range(n_states)\n",
    "                                                                                                                                                           \n",
    "    \n",
    "    selected_pairs_by_state = []\n",
    "    for state in chosen_states:\n",
    "        all_frames = [np.where(a == state)[0] for a in sequences]\n",
    "        pairs = [(trj, frame) for (trj, frames) in enumerate(all_frames) for frame in frames]\n",
    "        if random==False:\n",
    "            pair_deviation = [np.sqrt(np.sum(np.subtract(tica_transformed[pair[0]][pair[1]], HMM.means_[state])**2)) for pair in pairs]\n",
    "            pairs = np.asarray(pairs)[np.where(pair_deviation < np.median(pair_deviation))]\n",
    "        if random==-1:\n",
    "            pair_deviation = [np.sqrt(np.sum(np.subtract(tica_transformed[pair[0]][pair[1]], HMM.means_[state])**2)) for pair in pairs]\n",
    "            pairs = np.asarray(pairs)[np.where(pair_deviation > np.percentile(pair_deviation,95))]\n",
    "            \n",
    "        selected_pairs_by_state.append([pairs[random_state.choice(len(pairs))] for i in range(n_samples)])                                                                                                                                                      \n",
    "    return np.array(selected_pairs_by_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test timescales for a given number of states\n",
    "plot_HMM_timescales(tica_transformed, n_states=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test n_states for a given lagtime\n",
    "lag_time=1000\n",
    "strided_data = [s[i::lag_time] for s in tica_transformed for i in range(0,lag_time,10)]\n",
    "logprob_list = []\n",
    "for n_states in [2,4,6,8]:\n",
    "    print(\"n_states = \"+str(n_states))\n",
    "    HMM = hmm.GaussianHMM(n_states=n_states, n_init=200, n_iter=50).fit(strided_data)\n",
    "    logprob_list.append(HMM.fit_logprob_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logprob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#When you decide on the number of states and lag time for your HMM, this will build it and write output\n",
    "n_states = 8\n",
    "lag_time = 700\n",
    "\n",
    "file_path = pass_folder+\"/HMM/\"\n",
    "os.makedirs(file_path)\n",
    "#tica_transformed = utils.load(pass_folder+\"tica_transformed\")\n",
    "strided_data = [s[l::lag_time] for s in tica_transformed for l in range(lag_time)]\n",
    "print(\"Starting HMM\")\n",
    "HMM = hmm.GaussianHMM(n_states=n_states, n_init=30, n_iter=100).fit(strided_data)\n",
    "utils.dump(HMM, file_path+\"HMM\")\n",
    "print(\"HMM created\")\n",
    "\n",
    "tProb = HMM.transmat_\n",
    "tProbFile=open(file_path+\"/tProb.mtx\", \"w\")\n",
    "line = ' '.join(map(str,np.shape(tProb))) + ' ' + str(np.count_nonzero(tProb)) + '\\n'\n",
    "tProbFile.write(line)\n",
    "for indexes, value in np.ndenumerate(tProb):\n",
    "    x,y = indexes\n",
    "    if (value != 0):\n",
    "        line = str(x+1) + ' ' + str(y+1) + ' ' + str(value) + \"\\n\"\n",
    "        tProbFile.write(line)\n",
    "\n",
    "tProbFile.close()\n",
    "\n",
    "np.savetxt(file_path+'/Populations.dat', HMM.populations_, delimiter='\\n')\n",
    "\n",
    "selected_pairs_by_state = HMM.draw_centroids(np.asarray(tica_transformed))\n",
    "selected_pairs_by_state = selected_pairs_by_state[0].squeeze()\n",
    "for state, pair in enumerate(selected_pairs_by_state):\n",
    "#    pair[0] = mapping[pair[0]]\n",
    "    samples = utils.map_drawn_samples([[pair]], ds)\n",
    "    print(state,)\n",
    "    filename = str(file_path)+'/Mean' + str(state) + '.xtc'\n",
    "    samples[0].save_xtc(filename)\n",
    "\n",
    "selected_pairs_by_state = HMM_draw_samples(HMM, tica_transformed, 10)\n",
    "for state, samples in enumerate(selected_pairs_by_state):\n",
    "    traj = None\n",
    "    for pair_num, pair in enumerate(samples):\n",
    "        print(pair_num,)\n",
    "        traj_num,frame = pair\n",
    "        traj_num = traj_num\n",
    "#        traj_num = mapping[traj_num]\n",
    "        try:\n",
    "            traj = traj.join(ds.get(traj_num)[frame])\n",
    "        except:\n",
    "            traj = ds.get(traj_num)[frame]\n",
    "    filename = str(file_path)+'/State' + str(state) + '.xtc'\n",
    "    traj.save_xtc(filename)\n",
    "    print(state,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
